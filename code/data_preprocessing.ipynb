{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103cee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import psutil  \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import squarify\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b63c8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe original file is in JSON format, and we will convert it into CSV format and \\nsave it as \"processed_sports_facilities_11.csv\" for easier management in the future.\\n\\nSince some of the naming in this data is ambiguous and fragmented, we will categorize \\nfacilities with the same function into groups to streamline subsequent management. \\nFor example, \\'BA10\\', \\'BA11\\', and \\'BA12\\' all represent badminton courts, so we will \\nclassify them under the same category.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The original file is in JSON format, and we will convert it into CSV format and \n",
    "save it as \"processed_sports_facilities_11.csv\" for easier management in the future.\n",
    "\n",
    "Since some of the naming in this data is ambiguous and fragmented, we will categorize \n",
    "facilities with the same function into groups to streamline subsequent management. \n",
    "For example, 'BA10', 'BA11', and 'BA12' all represent badminton courts, so we will \n",
    "classify them under the same category.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f82bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in file: 611253\n",
      "Starting file processing: sports_facilities_booking_2023.json\n",
      "Chunk size: 5000 records\n",
      "Memory usage: 171.83 MB\n",
      "Processed: 10000/611253 lines | Progress: 1.6% | Speed: 83812 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 20000/611253 lines | Progress: 3.3% | Speed: 132480 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 30000/611253 lines | Progress: 4.9% | Speed: 163415 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 40000/611253 lines | Progress: 6.5% | Speed: 184806 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 50000/611253 lines | Progress: 8.2% | Speed: 200783 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 60000/611253 lines | Progress: 9.8% | Speed: 213934 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 70000/611253 lines | Progress: 11.5% | Speed: 223569 lines/second\n",
      "Memory usage: 175.55 MB\n",
      "Processed: 80000/611253 lines | Progress: 13.1% | Speed: 212095 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 90000/611253 lines | Progress: 14.7% | Speed: 219697 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 100000/611253 lines | Progress: 16.4% | Speed: 226302 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 110000/611253 lines | Progress: 18.0% | Speed: 232179 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 120000/611253 lines | Progress: 19.6% | Speed: 237371 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 130000/611253 lines | Progress: 21.3% | Speed: 241084 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 140000/611253 lines | Progress: 22.9% | Speed: 244698 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 150000/611253 lines | Progress: 24.5% | Speed: 248290 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 160000/611253 lines | Progress: 26.2% | Speed: 251578 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 170000/611253 lines | Progress: 27.8% | Speed: 254472 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 180000/611253 lines | Progress: 29.4% | Speed: 257178 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 190000/611253 lines | Progress: 31.1% | Speed: 259115 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 200000/611253 lines | Progress: 32.7% | Speed: 261150 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 210000/611253 lines | Progress: 34.4% | Speed: 262987 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 220000/611253 lines | Progress: 36.0% | Speed: 264698 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 230000/611253 lines | Progress: 37.6% | Speed: 266522 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 240000/611253 lines | Progress: 39.3% | Speed: 268238 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 250000/611253 lines | Progress: 40.9% | Speed: 269629 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 260000/611253 lines | Progress: 42.5% | Speed: 271082 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 270000/611253 lines | Progress: 44.2% | Speed: 272004 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 280000/611253 lines | Progress: 45.8% | Speed: 272280 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 290000/611253 lines | Progress: 47.4% | Speed: 273534 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 300000/611253 lines | Progress: 49.1% | Speed: 274742 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 310000/611253 lines | Progress: 50.7% | Speed: 275893 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 320000/611253 lines | Progress: 52.4% | Speed: 276923 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 330000/611253 lines | Progress: 54.0% | Speed: 277937 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 340000/611253 lines | Progress: 55.6% | Speed: 278529 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 350000/611253 lines | Progress: 57.3% | Speed: 279043 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 360000/611253 lines | Progress: 58.9% | Speed: 279683 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 370000/611253 lines | Progress: 60.5% | Speed: 280460 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 380000/611253 lines | Progress: 62.2% | Speed: 281155 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 390000/611253 lines | Progress: 63.8% | Speed: 281978 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 400000/611253 lines | Progress: 65.4% | Speed: 282675 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 410000/611253 lines | Progress: 67.1% | Speed: 283212 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 420000/611253 lines | Progress: 68.7% | Speed: 283594 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 430000/611253 lines | Progress: 70.3% | Speed: 284106 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 440000/611253 lines | Progress: 72.0% | Speed: 284597 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 450000/611253 lines | Progress: 73.6% | Speed: 284811 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 460000/611253 lines | Progress: 75.3% | Speed: 285282 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 470000/611253 lines | Progress: 76.9% | Speed: 285622 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 480000/611253 lines | Progress: 78.5% | Speed: 286065 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 490000/611253 lines | Progress: 80.2% | Speed: 286258 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 500000/611253 lines | Progress: 81.8% | Speed: 286639 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 510000/611253 lines | Progress: 83.4% | Speed: 287048 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 520000/611253 lines | Progress: 85.1% | Speed: 287489 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 530000/611253 lines | Progress: 86.7% | Speed: 287943 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 540000/611253 lines | Progress: 88.3% | Speed: 288198 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 550000/611253 lines | Progress: 90.0% | Speed: 288568 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 560000/611253 lines | Progress: 91.6% | Speed: 288661 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 570000/611253 lines | Progress: 93.3% | Speed: 288956 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 580000/611253 lines | Progress: 94.9% | Speed: 289262 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 590000/611253 lines | Progress: 96.5% | Speed: 289545 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 600000/611253 lines | Progress: 98.2% | Speed: 289863 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "Processed: 610000/611253 lines | Progress: 99.8% | Speed: 290120 lines/second\n",
      "Memory usage: 175.56 MB\n",
      "\n",
      "✅ Processing completed!\n",
      "Total processed records: 611253\n",
      "Total data chunks: 122\n",
      "Total time taken: 2.11 seconds\n",
      "Average speed: 289011.80 records/second\n",
      "CSV file successfully created: processed_sports_facilities_11.csv\n",
      "File size: 55.50 MB\n"
     ]
    }
   ],
   "source": [
    "def process_large_json_to_csv(input_file_path, output_csv_path, chunk_size=5000):\n",
    "\n",
    "    \n",
    "    def extract_nested_values(record):\n",
    "        try:\n",
    "            \n",
    "            processed = {\n",
    "                '_id': record.get('_id', {}).get('$oid', ''),\n",
    "                'bookingID': int(record.get('bookingID', {}).get('$numberLong', 0)),\n",
    "                'bookingDate': record.get('bookingDate', {}).get('$date', ''),\n",
    "                'placeCode': record.get('placeCode', ''),\n",
    "                'timeSlot': int(record.get('timeSlot', {}).get('$numberLong', 0)),\n",
    "                'bookingStatus': record.get('bookingStatus', ''),\n",
    "                'lastModifiedDate': record.get('lastModifiedDate', {}).get('$date', '')\n",
    "            }\n",
    "            return processed\n",
    "        except Exception as e:\n",
    "            print(f\"Record processing error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def print_memory_usage():\n",
    "        \"\"\"Print current memory usage\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem = process.memory_info().rss / (1024 * 1024)  \n",
    "        print(f\"Memory usage: {mem:.2f} MB\")\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_records = 0\n",
    "    processed_records = 0\n",
    "    chunk_count = 0\n",
    "    current_chunk = []\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            total_records = sum(1 for _ in f)\n",
    "        print(f\"Total records in file: {total_records}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot get total lines in file: {e}\")\n",
    "        total_records = 0\n",
    "    \n",
    "    print(f\"Starting file processing: {input_file_path}\")\n",
    "    print(f\"Chunk size: {chunk_size} records\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        with open(output_csv_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            fieldnames = [\n",
    "                '_id', 'bookingID', 'bookingDate', 'placeCode', \n",
    "                'timeSlot', 'bookingStatus', 'lastModifiedDate'\n",
    "            ]\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            csv_writer.writeheader()\n",
    "            \n",
    "            with open(input_file_path, 'r', encoding='utf-8') as json_file:\n",
    "                line_number = 0\n",
    "                \n",
    "                for line in json_file:\n",
    "                    line_number += 1\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    # Display progress\n",
    "                    if line_number % 10000 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        speed = line_number / elapsed if elapsed > 0 else 0\n",
    "                        print(f\"Processed: {line_number}/{total_records} lines | \"\n",
    "                              f\"Progress: {line_number/total_records*100:.1f}% | \"\n",
    "                              f\"Speed: {speed:.0f} lines/second\")\n",
    "                        print_memory_usage()\n",
    "                    \n",
    "                    try:\n",
    "                        # Parse JSON line\n",
    "                        record = json.loads(line)\n",
    "                        # Process nested values\n",
    "                        processed_record = extract_nested_values(record)\n",
    "                        \n",
    "                        if processed_record:\n",
    "                            current_chunk.append(processed_record)\n",
    "                            processed_records += 1\n",
    "                        \n",
    "                        # Write to CSV when chunk size is reached\n",
    "                        if len(current_chunk) >= chunk_size:\n",
    "                            chunk_count += 1\n",
    "                            csv_writer.writerows(current_chunk)\n",
    "                            current_chunk = []\n",
    "                    \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Line {line_number} JSON parsing error: {e}\")\n",
    "                        print(f\"Problematic line content: {line[:200]}...\")  # Show first 200 characters\n",
    "                    except Exception as e:\n",
    "                        print(f\"Line {line_number} processing error: {e}\")\n",
    "            \n",
    "            # Process the last incomplete chunk\n",
    "            if current_chunk:\n",
    "                csv_writer.writerows(current_chunk)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found {input_file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate total statistics\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n✅ Processing completed!\")\n",
    "    print(f\"Total processed records: {processed_records}\")\n",
    "    print(f\"Total data chunks: {chunk_count}\")\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if total_time > 0:\n",
    "        print(f\"Average speed: {processed_records/total_time:.2f} records/second\")\n",
    "    \n",
    "    return output_csv_path\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure file paths\n",
    "    input_json_file = \"../data/sports_facilities_booking_2023.json\"  # Replace with your JSON file path\n",
    "    output_csv_file = \"../data/processed_sports_facilities_11.csv\"      # Output CSV file\n",
    "    \n",
    "    # Install required libraries (if not installed)\n",
    "    try:\n",
    "        import psutil\n",
    "    except ImportError:\n",
    "        print(\"Installing psutil for memory monitoring...\")\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"psutil\"])\n",
    "        import psutil\n",
    "    \n",
    "    # Execute conversion\n",
    "    processed_csv = process_large_json_to_csv(\n",
    "        input_file_path=input_json_file,\n",
    "        output_csv_path=output_csv_file,\n",
    "        chunk_size=5000  # Smaller chunk size is safer\n",
    "    )\n",
    "    \n",
    "    if processed_csv:\n",
    "        print(f\"CSV file successfully created: {processed_csv}\")\n",
    "        print(f\"File size: {os.path.getsize(processed_csv)/(1024 * 1024):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
